{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, LSTM, Dropout, Input\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n",
      "2.0.8\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load ascii text and covert to lowercase\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make look up tables based on characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144345\n",
      "Total Unique chars:  46\n"
     ]
    }
   ],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Unique chars: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#raw_text = raw_text[1:]\n",
    "len(int_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the sequences\n",
    "\n",
    "This shows you an example of making sequences sampled from the overall text data. \n",
    "\n",
    "We are creating sequences that are 100 characters long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sequences:  144235\n"
     ]
    }
   ],
   "source": [
    "# create input and output pairs\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "dataZ = [] #\n",
    "for i in range(0, n_chars - seq_length - 10, 1): # 10 for analysis of the tail of the sequence\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    seq_out_extended = raw_text[(i + seq_length):(i + seq_length + 10)]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    dataZ.append([char_to_int[char] for char in seq_out_extended])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total sequences: \", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets examine some of these sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44, 43, 45, 19, 24, 17, 32, 36, 21, 34, 1, 25, 10, 1, 20, 31, 39, 30, 1, 36, 24, 21, 1, 34, 17, 18, 18, 25, 36, 9, 24, 31, 28, 21, 0, 0, 17, 28, 25, 19, 21, 1, 39, 17, 35, 1, 18, 21, 23, 25, 30, 30, 25, 30, 23, 1, 36, 31, 1, 23, 21, 36, 1, 38, 21, 34, 41, 1, 36, 25, 34, 21, 20, 1, 31, 22, 1, 35, 25, 36, 36, 25, 30, 23, 1, 18, 41, 1, 24, 21, 34, 1, 35, 25, 35, 36, 21, 34, 1, 31]\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "print(dataX[0])\n",
    "print(dataY[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30, 1, 36, 24, 21, 0, 18, 17, 30, 27]\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "print(dataZ[0])\n",
    "print(dataZ[0][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" п»їchapter i. down the rabbit-hole\n",
      "\n",
      "alice was beginning to get very tired of sitting by her sister o \"\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "print(\"\\\"\", ''.join([int_to_char[value] for value in dataX[0]]), \"\\\"\")\n",
    "print(int_to_char[dataY[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43, 45, 19, 24, 17, 32, 36, 21, 34, 1, 25, 10, 1, 20, 31, 39, 30, 1, 36, 24, 21, 1, 34, 17, 18, 18, 25, 36, 9, 24, 31, 28, 21, 0, 0, 17, 28, 25, 19, 21, 1, 39, 17, 35, 1, 18, 21, 23, 25, 30, 30, 25, 30, 23, 1, 36, 31, 1, 23, 21, 36, 1, 38, 21, 34, 41, 1, 36, 25, 34, 21, 20, 1, 31, 22, 1, 35, 25, 36, 36, 25, 30, 23, 1, 18, 41, 1, 24, 21, 34, 1, 35, 25, 35, 36, 21, 34, 1, 31, 30]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(dataX[1])\n",
    "print(dataY[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" »їchapter i. down the rabbit-hole\n",
      "\n",
      "alice was beginning to get very tired of sitting by her sister on \"\n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(\"\\\"\", ''.join([int_to_char[value] for value in dataX[1]]), \"\\\"\")\n",
    "print(int_to_char[dataY[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\" hapter i. down the rabbit-hole\n",
      "\n",
      "alice was beginning to get very tired of sitting by her sister on th \"\n",
      "e\n"
     ]
    }
   ],
   "source": [
    "print(\"\\\"\", ''.join([int_to_char[value] for value in dataX[4]]), \"\\\"\")\n",
    "print(int_to_char[dataY[4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping the sequences to become timesteps into the LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144235, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "print(X.shape)\n",
    "\n",
    "# normalize\n",
    "X = X / float(n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.95652174]\n",
      " [ 0.93478261]\n",
      " [ 0.97826087]\n",
      " [ 0.41304348]\n",
      " [ 0.52173913]\n",
      " [ 0.36956522]\n",
      " [ 0.69565217]\n",
      " [ 0.7826087 ]\n",
      " [ 0.45652174]\n",
      " [ 0.73913043]]\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "print(X[0][:10])\n",
    "print(dataY[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.95652174]\n",
      " [ 0.93478261]\n",
      " [ 0.97826087]\n",
      " [ 0.41304348]\n",
      " [ 0.52173913]\n",
      " [ 0.36956522]\n",
      " [ 0.69565217]\n",
      " [ 0.7826087 ]\n",
      " [ 0.45652174]\n",
      " [ 0.73913043]]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(X[0][:10])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating our model\n",
    "\n",
    "We will use the return sequences = true to pass the sequence up to the 2nd LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144235, 100, 1)\n",
      "our input shape is  (100, 1)\n"
     ]
    }
   ],
   "source": [
    "# define the input shape\n",
    "print(X.shape)\n",
    "inp = Input(shape=(X.shape[1], X.shape[2]))\n",
    "print('our input shape is ',(X.shape[1], X.shape[2]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = LSTM(256, return_sequences = True)(inp) \n",
    "#x = Dropout(0.2)(x)\n",
    "x = LSTM(256)(x)\n",
    "#x = Dropout(0.2)(x)\n",
    "output = Dense(y.shape[1], activation ='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generative_model = Model(inputs = inp, outputs=output )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = RMSprop(lr=0.01)\n",
    "generative_model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-gentext-CharRNN.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "144192/144235 [============================>.] - ETA: 0s - loss: 2.2892Epoch 00000: loss improved from inf to 2.28918, saving model to weights-improvement-00-2.2892-gentext-CharRNN.hdf5\n",
      "144235/144235 [==============================] - 1638s - loss: 2.2892  \n",
      "Epoch 2/10\n",
      "144192/144235 [============================>.] - ETA: 0s - loss: 2.0584Epoch 00001: loss improved from 2.28918 to 2.05839, saving model to weights-improvement-01-2.0584-gentext-CharRNN.hdf5\n",
      "144235/144235 [==============================] - 1718s - loss: 2.0584  \n",
      "Epoch 3/10\n",
      "144192/144235 [============================>.] - ETA: 0s - loss: 1.9164Epoch 00002: loss improved from 2.05839 to 1.91642, saving model to weights-improvement-02-1.9164-gentext-CharRNN.hdf5\n",
      "144235/144235 [==============================] - 1723s - loss: 1.9164  \n",
      "Epoch 4/10\n",
      "144192/144235 [============================>.] - ETA: 0s - loss: 1.8129Epoch 00003: loss improved from 1.91642 to 1.81298, saving model to weights-improvement-03-1.8130-gentext-CharRNN.hdf5\n",
      "144235/144235 [==============================] - 1709s - loss: 1.8130  \n",
      "Epoch 5/10\n",
      "144192/144235 [============================>.] - ETA: 0s - loss: 1.7315Epoch 00004: loss improved from 1.81298 to 1.73136, saving model to weights-improvement-04-1.7314-gentext-CharRNN.hdf5\n",
      "144235/144235 [==============================] - 1729s - loss: 1.7314  \n",
      "Epoch 6/10\n",
      " 10048/144235 [=>............................] - ETA: 1452s - loss: 1.6779"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-7a2e2c63f85c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgenerative_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1430\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1432\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1079\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1080\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2268\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2269\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generative_model.fit(X, y, epochs=10, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generative_model.save('Text_gen_01-CharRNN_no_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.7414Epoch 00000: loss improved from 1.90994 to 1.74151, saving model to checkpoints/weights-improvement-00-1.7415-gentext-CharRNN.hdf5\n",
      "144243/144243 [==============================] - 313s - loss: 1.7415   \n",
      "Epoch 2/10\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.6662Epoch 00001: loss improved from 1.74151 to 1.66620, saving model to checkpoints/weights-improvement-01-1.6662-gentext-CharRNN.hdf5\n",
      "144243/144243 [==============================] - 313s - loss: 1.6662   \n",
      "Epoch 3/10\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.6029Epoch 00002: loss improved from 1.66620 to 1.60287, saving model to checkpoints/weights-improvement-02-1.6029-gentext-CharRNN.hdf5\n",
      "144243/144243 [==============================] - 313s - loss: 1.6029   \n",
      "Epoch 4/10\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.5466Epoch 00003: loss improved from 1.60287 to 1.54662, saving model to checkpoints/weights-improvement-03-1.5466-gentext-CharRNN.hdf5\n",
      "144243/144243 [==============================] - 313s - loss: 1.5466   \n",
      "Epoch 5/10\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.4981Epoch 00004: loss improved from 1.54662 to 1.49814, saving model to checkpoints/weights-improvement-04-1.4981-gentext-CharRNN.hdf5\n",
      "144243/144243 [==============================] - 313s - loss: 1.4981   \n",
      "Epoch 6/10\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.4513Epoch 00005: loss improved from 1.49814 to 1.45128, saving model to checkpoints/weights-improvement-05-1.4513-gentext-CharRNN.hdf5\n",
      "144243/144243 [==============================] - 314s - loss: 1.4513   \n",
      "Epoch 7/10\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.4078Epoch 00006: loss improved from 1.45128 to 1.40787, saving model to checkpoints/weights-improvement-06-1.4079-gentext-CharRNN.hdf5\n",
      "144243/144243 [==============================] - 313s - loss: 1.4079   \n",
      "Epoch 8/10\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.3701Epoch 00007: loss improved from 1.40787 to 1.37014, saving model to checkpoints/weights-improvement-07-1.3701-gentext-CharRNN.hdf5\n",
      "144243/144243 [==============================] - 313s - loss: 1.3701   \n",
      "Epoch 9/10\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.3343Epoch 00008: loss improved from 1.37014 to 1.33446, saving model to checkpoints/weights-improvement-08-1.3345-gentext-CharRNN.hdf5\n",
      "144243/144243 [==============================] - 314s - loss: 1.3345   \n",
      "Epoch 10/10\n",
      "144192/144243 [============================>.] - ETA: 0s - loss: 1.3005Epoch 00009: loss improved from 1.33446 to 1.30045, saving model to checkpoints/weights-improvement-09-1.3005-gentext-CharRNN.hdf5\n",
      "144243/144243 [==============================] - 313s - loss: 1.3005   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fdb7d5035c0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generative_model.fit(X, y, epochs=10, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generative_model.save('Text_gen_01-CharRNN_no_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generative_model = load_model('weights-improvement-04-1.7314-gentext-CharRNN.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144345\n",
      "Total Vocab:  46\n"
     ]
    }
   ],
   "source": [
    "# create mapping of unique chars to integers, and a reverse mapping\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/cpu:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 9602891339040562691\n",
      ", name: \"/gpu:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 88070553\n",
      "locality {\n",
      "  bus_id: 1\n",
      "}\n",
      "incarnation: 17996356155113496566\n",
      "physical_device_desc: \"device: 0, name: GeForce GT 640, pci bus id: 0000:01:00.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataZZ  =dataZ.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init  0\n",
      "init  10\n",
      "init  20\n",
      "init  30\n",
      "init  40\n",
      "init  50\n",
      "init  60\n",
      "init  70\n",
      "init  80\n",
      "init  90\n",
      "init  100\n",
      "init  110\n",
      "init  120\n",
      "init  130\n",
      "init  140\n",
      "init  150\n",
      "init  160\n",
      "init  170\n",
      "init  180\n",
      "init  190\n",
      "init  200\n",
      "init  210\n",
      "init  220\n",
      "init  230\n",
      "init  240\n",
      "init  250\n",
      "init  260\n",
      "init  270\n",
      "init  280\n",
      "init  290\n",
      "init  300\n",
      "init  310\n",
      "init  320\n",
      "init  330\n",
      "init  340\n",
      "init  350\n",
      "init  360\n",
      "init  370\n",
      "init  380\n",
      "init  390\n",
      "init  400\n",
      "init  410\n",
      "init  420\n",
      "init  430\n",
      "init  440\n",
      "init  450\n",
      "init  460\n",
      "init  470\n",
      "init  480\n",
      "init  490\n",
      "init  500\n",
      "init  510\n",
      "init  520\n",
      "init  530\n",
      "init  540\n",
      "init  550\n",
      "init  560\n",
      "init  570\n",
      "init  580\n",
      "init  590\n",
      "init  600\n",
      "init  610\n",
      "init  620\n",
      "init  630\n",
      "init  640\n",
      "init  650\n",
      "init  660\n",
      "init  670\n",
      "init  680\n",
      "init  690\n",
      "init  700\n",
      "init  710\n",
      "init  720\n",
      "init  730\n",
      "init  740\n",
      "init  750\n",
      "init  760\n",
      "init  770\n",
      "init  780\n",
      "init  790\n",
      "init  800\n",
      "init  810\n",
      "init  820\n",
      "init  830\n",
      "init  840\n",
      "init  850\n",
      "init  860\n",
      "init  870\n",
      "init  880\n",
      "init  890\n",
      "init  900\n",
      "init  910\n",
      "init  920\n",
      "init  930\n",
      "init  940\n",
      "init  950\n",
      "init  960\n",
      "init  970\n",
      "init  980\n",
      "init  990\n",
      "init  1000\n",
      "init  1010\n",
      "init  1020\n",
      "init  1030\n",
      "init  1040\n",
      "init  1050\n",
      "init  1060\n",
      "init  1070\n",
      "init  1080\n",
      "init  1090\n",
      "init  1100\n",
      "init  1110\n",
      "init  1120\n",
      "init  1130\n",
      "init  1140\n",
      "init  1150\n",
      "init  1160\n",
      "init  1170\n",
      "init  1180\n",
      "init  1190\n",
      "init  1200\n",
      "init  1210\n",
      "init  1220\n",
      "init  1230\n",
      "init  1240\n",
      "init  1250\n",
      "init  1260\n",
      "init  1270\n",
      "init  1280\n",
      "init  1290\n",
      "init  1300\n",
      "init  1310\n",
      "init  1320\n",
      "init  1330\n",
      "init  1340\n",
      "init  1350\n",
      "init  1360\n",
      "init  1370\n",
      "init  1380\n",
      "init  1390\n",
      "init  1400\n",
      "init  1410\n",
      "init  1420\n",
      "init  1430\n",
      "init  1440\n",
      "init  1450\n",
      "init  1460\n",
      "init  1470\n",
      "init  1480\n",
      "init  1490\n",
      "init  1500\n",
      "init  1510\n",
      "init  1520\n",
      "init  1530\n",
      "init  1540\n",
      "init  1550\n",
      "init  1560\n",
      "init  1570\n",
      "init  1580\n",
      "init  1590\n",
      "init  1600\n",
      "init  1610\n",
      "init  1620\n",
      "init  1630\n",
      "init  1640\n",
      "init  1650\n",
      "init  1660\n",
      "init  1670\n",
      "init  1680\n",
      "init  1690\n",
      "init  1700\n",
      "init  1710\n",
      "init  1720\n",
      "init  1730\n",
      "init  1740\n",
      "init  1750\n",
      "init  1760\n",
      "init  1770\n",
      "init  1780\n",
      "init  1790\n",
      "init  1800\n",
      "init  1810\n",
      "init  1820\n",
      "init  1830\n",
      "init  1840\n",
      "init  1850\n",
      "init  1860\n",
      "init  1870\n",
      "init  1880\n",
      "init  1890\n",
      "init  1900\n",
      "init  1910\n",
      "init  1920\n",
      "init  1930\n",
      "init  1940\n",
      "init  1950\n",
      "init  1960\n",
      "init  1970\n",
      "init  1980\n",
      "init  1990\n",
      "ResProb\n",
      " [ 0.5035  0.366   0.256   0.195   0.1535  0.1185  0.102   0.0915  0.0895\n",
      "  0.082 ]\n"
     ]
    }
   ],
   "source": [
    "all_patterns = n_chars - seq_length - 10\n",
    "ResProb = np.zeros(10) # to calculate the correct number of generated symbols\n",
    "ResProbA = np.zeros(10) # to calculate the correct number of generated symbols if previous one was correct\n",
    "\n",
    "\n",
    "all_patterns = 2000\n",
    "\n",
    "for i in range(0, all_patterns, 1): # 10 for analysis of the tail of the sequence\n",
    "#for i in range(0, 5, 1): # 10 for analysis of the tail of the sequence\n",
    "    \n",
    "    pattern = dataX[i].copy()\n",
    "    seed = dataX[i]\n",
    "    res = dataZ[i]\n",
    "    \n",
    "    prev = 1 # previous symbol was the correct one\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(\"init \", i)\n",
    "    \n",
    "    # generate characters\n",
    "    for j in range(10):\n",
    "        x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        x = x / float(n_vocab)\n",
    "        prediction = generative_model.predict(x, verbose=0)\n",
    "        index = np.argmax(prediction)\n",
    "        if index==res[j]:\n",
    "            ResProb[j]+=1\n",
    "            if prev == 1:\n",
    "                ResProbA[j]+=1\n",
    "            prev = 1\n",
    "        else:\n",
    "            prev = 0\n",
    "                \n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    " #       print(len(pattern))\n",
    "\n",
    "\n",
    "ResProb = ResProb / all_patterns # to calculate the probability\n",
    "ResProbA = ResProbA / all_patterns # to calculate the probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResProb\n",
      " [ 0.5035  0.366   0.256   0.195   0.1535  0.1185  0.102   0.0915  0.0895\n",
      "  0.082 ]\n",
      "ResProbA\n",
      " [ 0.5035  0.3035  0.1985  0.121   0.081   0.054   0.036   0.024   0.016\n",
      "  0.0185]\n"
     ]
    }
   ],
   "source": [
    "print(\"ResProb\\n\", ResProb)\n",
    "print(\"ResProbA\\n\", ResProbA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPmeCbAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x22f71687358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.plot(ResProb, 'bo')\n",
    "pyplot.plot(ResProbA, 'ro')\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    }
   ],
   "source": [
    "print(len(dataX[0]) )     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "[45, 43, 44, 19, 24, 17, 32, 36, 21, 34, 1, 25, 10, 1, 20, 31, 39, 30, 1, 36, 24, 21, 1, 34, 17, 18, 18, 25, 36, 9, 24, 31, 28, 21, 0, 0, 17, 28, 25, 19, 21, 1, 39, 17, 35, 1, 18, 21, 23, 25, 30, 30, 25, 30, 23, 1, 36, 31, 1, 23, 21, 36, 1, 38, 21, 34, 41, 1, 36, 25, 34, 21, 20, 1, 31, 22, 1, 35, 25, 36, 36, 25, 30, 23, 1, 18, 41, 1, 24, 21, 34, 1, 35, 25, 35, 36, 21, 34, 1, 31, 22]\n",
      "Seed pattern:\n",
      "\" ï»¿chapter i. down the rabbit-hole\n",
      "\n",
      "alice was beginning to get very tired of sitting by her sister of \"\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "#start = 0\n",
    "pattern = dataX[start]\n",
    "print(len(pattern))\n",
    "seed = dataX[start]\n",
    "print(pattern)\n",
    "print(\"Seed pattern:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "generated_text = []\n",
    "\n",
    "# generate characters\n",
    "for i in range(100):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = generative_model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    pattern.append(index)\n",
    "    generated_text.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 30, 21, 1, 17, 18, 31, 37, 36, 1, 36, 24, 21, 1, 39, 24, 25, 36, 21, 1, 23, 17, 20, 1, 17, 1, 18, 31, 30, 27, 1, 36, 34, 1, 36, 24, 21, 1, 39, 31, 34, 20, 35, 1, 4, 1, 0, 4, 25, 1, 20, 31, 30, 4, 36, 1, 27, 30, 31, 39, 1, 36, 24, 21, 1, 39, 24, 25, 36, 21, 1, 34, 37, 21, 21, 34, 1, 35, 24, 21, 1, 39, 17, 41, 1, 25, 36, 1, 25, 17, 20, 1, 36, 31, 1, 35, 21, 21, 1, 36]\n",
      "\" , that makes the world go round!\"'\n",
      "\n",
      "'somebody said,' alice whispered, 'that it's done by everybody mi \"\n",
      "\" ine about the white gad a bonk tr the words ' \n",
      "'i don't know the white rueer she way it iad to see t \"\n"
     ]
    }
   ],
   "source": [
    "print(pattern)\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in seed]), \"\\\"\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in generated_text]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 21, 1, 39, 17, 41, 1, 25, 30, 36, 31, 1, 36, 24, 21, 1, 21, 17, 30, 19, 21, 13, 0, 1, 1, 39, 31, 37, 28, 20, 1, 41, 31, 37, 1, 36, 21, 28, 28, 1, 29, 21, 1, 41, 31, 37, 1, 23, 17, 20, 1, 36, 31, 1, 35, 21, 21, 1, 36, 24, 21, 1, 39, 17, 41, 1, 25, 30, 1, 36, 24, 21, 1, 39, 17, 41, 1, 25, 30, 1, 36, 24, 21, 1, 39, 17, 41, 1, 25, 30, 1, 36, 24, 21, 1, 39, 17, 41, 1, 25]\n",
      "\" , that makes the world go round!\"'\n",
      "\n",
      "'somebody said,' alice whispered, 'that it's done by everybody mi \"\n",
      "\" he way into the eance?\n",
      "  would you tell me you gad to see the way in the way in the way in the way i \"\n"
     ]
    }
   ],
   "source": [
    "generated_text = []\n",
    "\n",
    "# generate characters\n",
    "for i in range(100):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = generative_model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    pattern.append(index)\n",
    "    generated_text.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(pattern)\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in seed]), \"\\\"\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in generated_text]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 18, 21, 1, 17, 1, 32, 21, 36, 19, 34, 1, 24, 34, 31, 39, 1, 36, 24, 21, 1, 39, 24, 25, 36, 21, 1, 31, 17, 20, 21, 1, 31, 22, 1, 36, 24, 21, 1, 35, 31, 31, 28, 8, 1, 17, 30, 20, 1, 36, 24, 21, 1, 29, 31, 19, 35, 36, 21, 34, 35, 1, 25, 1, 24, 17, 38, 21, 1, 20, 31, 30, 21, 1, 36, 31, 1, 36, 24, 21, 1, 39, 31, 34, 20, 35, 1, 24, 31, 36, 1, 36, 31, 1, 36, 24, 21, 1, 39, 31]\n",
      "\" , that makes the world go round!\"'\n",
      "\n",
      "'somebody said,' alice whispered, 'that it's done by everybody mi \"\n",
      "\" n the way in the way of starpelte that she had to sto the white rabbit heard the white rabbit, who was she white rabbit reterely at the white rabbit, whth a soor lany and frownen to the words the whole party things at the way of say that she was not and seneating the white rabbit, who was she white rabbit reterpadly, 'i should think you con't know the white giddres,' she said to herself, 'it would be a petcr hrow the white oade of the sool, and the mocsters i have done to the words hot to the wo \"\n"
     ]
    }
   ],
   "source": [
    "generated_text = []\n",
    "\n",
    "# generate characters\n",
    "for i in range(500):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = generative_model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    pattern.append(index)\n",
    "    generated_text.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(pattern)\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in seed]), \"\\\"\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in generated_text]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
